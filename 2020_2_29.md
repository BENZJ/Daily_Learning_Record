# 今日计划
- 完成deepfool代码攻击Lenet网络在Mnist数据库实验
- 重新看一遍通用模版文章和代码
- 看几篇硕博士论文

# 收获
## vscode快捷键
代码收缩 先 cmd+k 再按 cmd+0
代码展开 先 cmd+k 再按 cmd+j

打开隐藏 终端 cmd+j
打开隐藏 边栏 cmd+b
## pytorch中变量复制的到底是引用还是创建了新对象
### python 中 .data 和 原先的变量区别
pytorch 中的这个内容真的是变态啊，x.data修改有问题具体看下面参考链接
```python
x = torch.ones(2, 2, requires_grad=True)

print(x)
print(x.data)
# 我们可以发现是一样的

print(x is x.data)
# False  说明两个并不是一个引用

print(x.data.requires_grad)
# 发现是false 
```
#### 链接
https://discuss.pytorch.org/t/resolved-how-does-one-create-copy-of-tensors-in-pytorch/6121

https://blog.csdn.net/g11d111/article/details/80840310

### requires_grad问题
tensor加上requires_grad True参数后有什么不同可以用下面代码解释
```python
x = torch.rand(5, 2)
y = torch.ones(5, 2, requires_grad=True)
z = torch.sum(x+y)
z.backward()
print (x.grad)
print (y.grad)
# 输出如下
# None
# tensor([[1., 1.],
#         [1., 1.],
#         [1., 1.],
#         [1., 1.],
#         [1., 1.]])
```
说明只有标记了requires_grad 的变量 才会在backward()后计算出grad。没有的.grad将会是None

#### python格式化输出
[参考](https://blog.csdn.net/xiaotao_1/article/details/79701048)

### colab保活
[链接](https://medium.com/@shivamrawat_756/how-to-prevent-google-colab-from-disconnecting-717b88a128c0)

## 总结
今天实际只完成了用Deepfool来攻击Lenet的代码。